<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>facebook on My New Hugo Site</title>
    <link>https://www.frankliu.org/hugo/tags/facebook/</link>
    <description>Recent content in facebook on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 21 Mar 2020 07:21:42 -0700</lastBuildDate>
    
	<atom:link href="https://www.frankliu.org/hugo/tags/facebook/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tao</title>
      <link>https://www.frankliu.org/hugo/posts/facebook-tao/</link>
      <pubDate>Sat, 21 Mar 2020 07:21:42 -0700</pubDate>
      
      <guid>https://www.frankliu.org/hugo/posts/facebook-tao/</guid>
      <description>Aggregation difficulties  content tailored to each user filter item with privacy checks for each user impossible to aggregate and filter when content is crated resolve data dependencies and privacy check each time content is viewed pull vs push social graph?  extreme read demands on graph data store    Memcache  rapid deployment  data mapping cache invalidation client code that is deployed frequently   created abstractions for graphcs  r/w objects (nodes) associations (edges) direct access to MySQL deprecated for graph data tyles    Tao  service  implements objects and association model   motivation  encapsulating failures in the PHP API access graph easily from non-PHP serivces problems with lookaside cache architecture    Inefficient edge lists  KV cache is not good semantic fit for lists of edges  queries must fetch the entire edge list  list support would help   make changes to single edge causes entire list to be reloaded  requires coordination of incremental updates to cached lists      Distributed control logic  L-A $ control logic is run on clients  clients don&amp;rsquo;t communicate with each other increases the number of failure modes difficult to avoid thundering herds  Nishtala et al.</description>
    </item>
    
    <item>
      <title>Facebook - memcached</title>
      <link>https://www.frankliu.org/hugo/posts/facebook-memcached/</link>
      <pubDate>Sat, 07 Mar 2020 07:47:47 -0800</pubDate>
      
      <guid>https://www.frankliu.org/hugo/posts/facebook-memcached/</guid>
      <description>Requirements  real-time aggregate dispersed data access hot set scale refs [1,2,5,6,12,14,34,36]  Front-end cluster    read heavy workload (100:1 R/W) wide fanout handle failures 10 Mops/s  Q: what is a wide fanout
Multiple FE clusters  single geo region control data replication data consistency 100 Mops/s  Multiple regions  muliple geo regions storage replication data consistency 1 Bops/s  Pre-memcached   High fanout    data dependency graph for a small user request  Look-aside cache   why deletes over set</description>
    </item>
    
  </channel>
</rss>