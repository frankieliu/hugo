<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>distributed systems on My New Hugo Site</title>
    <link>https://www.frankliu.org/hugo/tags/distributed-systems/</link>
    <description>Recent content in distributed systems on My New Hugo Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 09 May 2020 10:08:17 -0700</lastBuildDate>
    
	<atom:link href="https://www.frankliu.org/hugo/tags/distributed-systems/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Google File System</title>
      <link>https://www.frankliu.org/hugo/posts/google-gfs/</link>
      <pubDate>Sat, 09 May 2020 10:08:17 -0700</pubDate>
      
      <guid>https://www.frankliu.org/hugo/posts/google-gfs/</guid>
      <description>5.1 High Availability Chunk Replication  RF of 3 master clones existing replicas when chunkservers go offline or detect corrupted replicas  Master Replication  operation log and checkpoints replicas &amp;ldquo;shadow&amp;rdquo; masters provide read-only access  file metadata like directory contest could be stale reads replica information from logs pools from chunkservers to locate chunk replicas depends on primary for decisions to create and delete replicas    5.2 Data Integrity  impractical to very replica data between replicas use 32bit checksum on 64KB blocks  stored persistently with logging and separate from user data   in reads:  chunkserver verifies the checksum before returning data client reads from another replica master creates a different replica and delete the corrupted one   in appends:  incrementally update the checksum for last partial checksum blocks even if last partial checksum is corrupted, new checksum value will not match stored data and corruption will be detected   in writes:  if write overwrites an existing range on the chunk, need to verify the first and last blocks of the range being overwritten calculate new checksums based from previous checksum so that corruption of unchanged areas will be detected    6 Measurements  1 master, two master replicas, 16 chunkservers, and 16 clients  6.</description>
    </item>
    
    <item>
      <title>Flink</title>
      <link>https://www.frankliu.org/hugo/posts/flink/</link>
      <pubDate>Sat, 18 Apr 2020 08:49:58 -0700</pubDate>
      
      <guid>https://www.frankliu.org/hugo/posts/flink/</guid>
      <description>What is it  a distributed runtime uses pipelines for execution exactly-once state consistency lightweight checkpoint iterative processing windows semantics out-of-order processing  2 architecture cluster  client job manager task manager (1 or more)  client  takes a program xforms to dataflow graph submits to job manager creates data schema and serializers cost-based query optmization  job manager  coordinates distributed execution of dataflow tracks state and progress of each operator and stream schedules new operators coordinates checkpoints and recovery persists minimal set of data for checkpoint and recovery  task manager  executes one or more operators report status to job manager maintain buffer pools to buffer or materialize streams maintain network connections to exchange of streams between operators    3.</description>
    </item>
    
    <item>
      <title>Facebook - memcached</title>
      <link>https://www.frankliu.org/hugo/posts/facebook-memcached/</link>
      <pubDate>Sat, 07 Mar 2020 07:47:47 -0800</pubDate>
      
      <guid>https://www.frankliu.org/hugo/posts/facebook-memcached/</guid>
      <description>Requirements  real-time aggregate dispersed data access hot set scale refs [1,2,5,6,12,14,34,36]  Front-end cluster    read heavy workload (100:1 R/W) wide fanout handle failures 10 Mops/s  Q: what is a wide fanout
Multiple FE clusters  single geo region control data replication data consistency 100 Mops/s  Multiple regions  muliple geo regions storage replication data consistency 1 Bops/s  Pre-memcached   High fanout    data dependency graph for a small user request  Look-aside cache   why deletes over set</description>
    </item>
    
  </channel>
</rss>